"""London_Smart_meter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GmMmqqN0x90dn-nW3utKJWY6ToqEGPHg
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os

from google.colab import drive
drive.mount('/content/drive')

# Define paths
project_dir = '/content/drive/My Drive/Elect_Project/electricity-weather'
notebook_name = 'London_Smart_meter.ipynb'
notebook_path = os.path.join(project_dir, notebook_name)

folder_path = '/content/drive/My Drive/Small LCL Data'
files = os.listdir(folder_path)
csv_files = [f for f in files if f.endswith('.csv')]
print(len(csv_files))

dfs = []
for files in csv_files:
    full_path = os.path.join(folder_path, files)
    df = pd.read_csv(full_path)
    dfs.append(df)

combined_data = pd.concat(dfs, ignore_index=True)
combined_data.head()
combined_data.head()

combined_data

combined_data.describe()

# Check for missing values in the entire DataFrame
missing_values = combined_data.isnull().sum()

# Display missing values for each column
print(missing_values)

# Convert DateTime column to datetime format
combined_data['DateTime'] = pd.to_datetime(combined_data['DateTime'])

# Split 'DateTime' into 'Date' and 'Time' columns
combined_data['Date'] = combined_data['DateTime'].dt.date
combined_data['Time'] = combined_data['DateTime'].dt.time

#rearranging columns
columns = ['LCLid', 'stdorToU', 'DateTime', 'Date', 'Time', 'KWH/hh (per half hour) ']
combined_data = combined_data[columns]
combined_data = combined_data.drop(columns=['DateTime'])
combined_data

#save the combined data frame
combined_data.to_csv('/content/drive/My Drive/meter_data.csv', index=False)

print(combined_data.columns)

# Convert 'KWH/hh (per half hour)' to numeric
combined_data['KWH/hh (per half hour) '] = pd.to_numeric(combined_data['KWH/hh (per half hour) '], errors='coerce')


# Group by 'LCLid', 'stdorToU', and 'Date' columns and sum the 'KWH/hh (per half hour)' column to get daily consumption
daily_consumption = combined_data.groupby(['LCLid', 'stdorToU', 'Date'], as_index=False)['KWH/hh (per half hour) '].sum()

# Rename the columns for clarity
daily_consumption.columns = ['LCLid', 'stdorToU', 'Date', 'Daily_Consumption_KWH']

# Display the first few rows of the daily consumption data
print(daily_consumption.head())

daily_consumption.to_csv('/content/drive/My Drive/daily_elect_data.csv', index=False)

daily_consumption

household_counts_sorted = daily_consumption['LCLid'].value_counts().sort_values(ascending=False)
print(household_counts_sorted)

# Group by LCLid and sum daily consumption
household_consumption = daily_consumption.groupby('LCLid')['Daily_Consumption_KWH'].sum().reset_index()
print(household_consumption)

# Group by tariff type
tariff = daily_consumption.groupby('stdorToU')['Daily_Consumption_KWH']

# Calculate mean daily consumption for each tariff type
mean_consumption = tariff.mean()

# Calculate median daily consumption for each tariff type
median_consumption = tariff.median()

# Calculate total consumption for each tariff type
total_consumption = tariff.sum()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x=total_consumption.index, y=total_consumption.values, palette='viridis')
plt.title('Total Daily Consumption by Tariff Type')
plt.xlabel('Tariff Type')
plt.ylabel('Mean Daily Consumption (KWH)')
plt.show()

from scipy import stats

# Perform t-test for mean daily consumption between Std and ToU tariffs
t_stat, p_value = stats.ttest_ind(tariff.get_group('Std'), tariff.get_group('ToU'), equal_var=False)
print(f"T-test results: t-statistic = {t_stat}, p-value = {p_value}")

import matplotlib.pyplot as plt

# Convert 'Date' to datetime format if it's not already
daily_consumption['Date'] = pd.to_datetime(daily_consumption['Date'])

# Time series plot
plt.figure(figsize=(12, 6))
plt.plot(daily_consumption['Date'], daily_consumption['Daily_Consumption_KWH'], marker='o', linestyle='-')
plt.title('Daily Consumption Over Time')
plt.xlabel('Date')
plt.ylabel('Daily Consumption (KWH)')
plt.grid(True)
plt.show()

import seaborn as sns

# Distribution plot
plt.figure(figsize=(10, 6))
sns.histplot(daily_consumption['Daily_Consumption_KWH'], bins=30, kde=True)
plt.title('Distribution of Daily Consumption')
plt.xlabel('Daily Consumption (KWH)')
plt.ylabel('Frequency')
plt.show()

# Box plot by stdorToU
plt.figure(figsize=(8, 6))
sns.boxplot(x='stdorToU', y='Daily_Consumption_KWH', data=daily_consumption)
plt.title('Daily Consumption by stdorToU')
plt.xlabel('stdorToU')
plt.ylabel('Daily Consumption (KWH)')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'Date' is already in datetime format in daily_consumption
daily_consumption['Month'] = daily_consumption['Date'].dt.month

# Seasonal plot across all years
plt.figure(figsize=(12, 6))
sns.lineplot(x='Month', y='Daily_Consumption_KWH', data=daily_consumption, ci=None)
plt.title('Seasonal Variation in Daily Consumption Across Months')
plt.xlabel('Month')
plt.ylabel('Daily Consumption (KWH)')
plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.grid(True)
plt.show()

"""**PREPROCESSING WEATHER DATA**"""

plt.figure(figsize=(10, 6))
sns.violinplot(x='stdorToU', y='Daily_Consumption_KWH', data=daily_consumption)
plt.title('Daily Consumption by stdorToU')
plt.xlabel('Type of Tariff (stdorToU)')
plt.ylabel('Daily Consumption (KWH)')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=daily_consumption, x='Daily_Consumption_KWH', hue='stdorToU', kde=True, bins=30)
plt.title('Distribution of Daily Consumption by stdorToU')
plt.xlabel('Daily Consumption (KWH)')
plt.ylabel('Frequency')
plt.legend(title='Type of Tariff (stdorToU)')
plt.show()

# Navigate into the cloned directory
#cloning github
!git clone https://github.com/greo234/electricity-weather.git

import os
os.chdir('electricity-weather')

# Verify the contents of the directory
!ls

# Unzip the weather data folder
import zipfile
with zipfile.ZipFile('weather_data.zip', 'r') as zip_ref:
    zip_ref.extractall('weather_data')

# Verifying the contents
!ls weather_data

# Read the first file separately to select specific columns
first_file = "weather_data/cloud_cover.txt"
df_first = pd.read_csv(first_file, skiprows=18, delimiter=',', skipinitialspace=True)

df_first

# List all files in the directory except the first file
all_files = [file for file in os.listdir('weather_data') if file.endswith('.txt') and file != 'cloud_cover.txt']

# Read PP column from each remaining file and store in a list of DataFrames
dfs = []
for file in all_files:
    file_path = os.path.join('weather_data', file)
    df = pd.read_csv(file_path, skiprows=18, usecols=[2], delimiter=',', skipinitialspace=True)
    dfs.append(df)

# Concatenate all DataFrames along columns (axis=1) to create the final DataFrame
final_df = pd.concat([df_first] + dfs, axis=1)

columns_to_drop = ['SOUID','Q_CC']
final_df.drop(columns_to_drop, axis=1, inplace=True)
final_df

# Create DataFrame
weather_df = pd.DataFrame(final_df)

# Convert DATE column to datetime format
weather_df['DATE'] = pd.to_datetime(weather_df['DATE'], format='%Y%m%d')

# Define the start and end dates for filtering
start_date = '2011-11-23'
end_date = '2014-02-28'

# Filter final_df based on the date range
filt_w = weather_df[(weather_df['DATE'] >= start_date) & (weather_df['DATE'] <= end_date)]
# Resetting the index to start from 0
filt_w.reset_index(drop=True, inplace=True)
filt_w

filt_w.to_csv('/content/drive/My Drive/weather_data.csv', index=False)

# Convert 'Date' column to datetime format in both DataFrames
daily_consumption['Date'] = pd.to_datetime(daily_consumption['Date'])
filt_w['DATE'] = pd.to_datetime(filt_w['DATE'])


# Merge household consumption and weather data on 'Date'
merged_df = pd.merge(daily_consumption, filt_w, left_on='Date', right_on='DATE', how='inner')

# Drop the duplicate 'DATE' column after merging if needed
merged_df.drop(columns=['DATE'], inplace=True)

# Display the merged DataFrame
print(merged_df.head())

merged_df

merged_df.to_csv('/content/drive/My Drive/electricity_weather.csv', index=False)

"""PART 2: Using My Preprocessed Dataset"""

daily_path = '/content/drive/My Drive/daily_elect_data.csv'
elect_path = '/content/drive/My Drive/meter_data.csv'
weather_path = '/content/drive/My Drive/weather_data.csv'
merged_path = '/content/drive/My Drive/electricity_weather.csv'
newea_path = 'https://raw.githubusercontent.com/greo234/electricity-weather/main/new_weather.csv'
hol_path = 'https://raw.githubusercontent.com/greo234/electricity-weather/main/ukbankholidays-jul19.csv'

new_data = pd.read_csv(newea_path)
# Convert 'Date' column to datetime format
new_data['date'] = pd.to_datetime(new_data['date'], format='%d/%m/%Y')

# Check for missing values
missing_values = new_data.isnull().sum()

print("Missing Values in Each Column:")
print(missing_values)

new_data['Tmin'] = new_data['tmin']*10
new_data['Tmax'] = new_data['tmax']*10

# Drop some old variable columns
columns_drop = ['tavg', 'snow', 'tmin', 'tmax', 'prcp', 'wpgt', 'tsun']

# dropping columns
new_df = new_data.drop(columns=columns_drop)

new_df

# Check for missing values
missing = new_df.isnull().sum()

print("Missing Values in Each Column:")
print(missing)

# Interpolation for missing values
new_df['wspd'] = new_df['wspd'].interpolate()
new_df['pres'] = new_df['pres'].interpolate()

# use bfill and ffill for wind pressure
new_df['wdir'] = new_df['wdir'].ffill().bfill()

new_df.describe()

new_df.rename(columns={'date': 'Date'}, inplace=True)
new_df

# loading holiday data
h = pd.read_csv(hol_path)
hols =h.rename(columns={'UK BANK HOLIDAYS': 'Date'}) #converting to date

hols

print(hols.columns)

#change to datfarmae
hols_df = pd.DataFrame(hols)

# Convert 'UK BANK HOLIDAYS' column to datetime format
hols_df['Date'] = pd.to_datetime(hols_df['Date'],
                                             format='%d-%b-%Y')
hols_df

merged_data = pd.read_csv(merged_path)
merged_data

merged_data.describe()

print(merged_data.dtypes)

# Ensure the 'Date' column is a datetime object
merged_data['Date'] = pd.to_datetime(merged_data['Date'])

# Group by 'Date' and count 'LCLid' to get the house count per date
housecount = merged_data.groupby('Date')[['LCLid']].count()
housecount.rename(columns={'LCLid': 'housecount'}, inplace=True)

# Sum the Daily Consumption KWH per date
energy_consumption = merged_data.groupby('Date')[['Daily_Consumption_KWH']].sum()

# Verify the result
print(housecount.head())
print(energy_consumption.head())

housecount.describe()

import pandas as pd

def get_season(date):
    """Assign a season to a given date."""
    month = date.month
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    elif month in [9, 10, 11]:
        return 'Autumn'

# Example usage with your dataset
def add_season_column(data):
    # Ensure 'Date' column is datetime type
    data['Date'] = pd.to_datetime(data['Date'])

    # Apply the get_season function to the 'Date' column
    data['Season'] = data['Date'].apply(get_season)

    return data

merge = add_season_column(merged_data)

merge

# Filter data for the optimal timeframe
optimal_start_date = '2012-06-01'
optimal_end_date = '2014-02-27'
data_optimal = merge[(merge['Date'] >= optimal_start_date) & (merge['Date'] <= optimal_end_date)]

merge

# Aggregate data by season
seasonal_consumption = merge.groupby('Season')['Daily_Consumption_KWH'].agg(['mean', 'sum']).reset_index()

print(seasonal_consumption)

import seaborn as sns

# Bar plot for average consumption by season
plt.figure(figsize=(10, 6), dpi=120)
sns.barplot(x='Season', y='mean', data=seasonal_consumption, palette='viridis')
plt.title('Average Daily Energy Consumption by Season')
plt.xlabel('Season')
plt.ylabel('Average Daily Consumption (KWH)')
plt.show()

# Bar plot for total consumption by season
plt.figure(figsize=(10, 6), dpi=120)
sns.barplot(x='Season', y='sum', data=seasonal_consumption, palette='viridis')
plt.title('Total Energy Consumption by Season')
plt.xlabel('Season')
plt.ylabel('Total Consumption (KWH)')
plt.show()

# Check for missing values
print("Missing values in merged_data:")
print(merged_data.isnull().sum())

# Check for outliers in Daily Consumption KWH
plt.figure(figsize=(10, 6))
plt.boxplot(merged_data['Daily_Consumption_KWH'])
plt.title('Box Plot of Daily Consumption KWH')
plt.ylabel('Daily Consumption (KWH)')
plt.grid(True)
plt.show()

# Explore temporal patterns
plt.figure(figsize=(14, 7))
plt.plot(merged_data['Date'], merged_data['Daily_Consumption_KWH'])
plt.title('Daily Consumption KWH Over Time')
plt.xlabel('Date')
plt.ylabel('Daily Consumption (KWH)')
plt.grid(True)
plt.show()

# Split the data based on 'stdorToU' column and sum the energy consumed
std_df = merged_data[merged_data['stdorToU'] == 'Std'].groupby(['LCLid', 'Date', 'CC', 'RR', 'TG', 'HU', 'SS', 'PP', 'SD', 'QQ'])['Daily_Consumption_KWH'].sum().reset_index()
tou_df = merged_data[merged_data['stdorToU'] == 'ToU'].groupby(['LCLid', 'Date', 'CC', 'RR', 'TG', 'HU', 'SS', 'PP', 'SD', 'QQ'])['Daily_Consumption_KWH'].sum().reset_index()

import pandas as pd

def dataframe(data):
    # Convert 'Date' column to datetime
    data['Date'] = pd.to_datetime(data['Date'])

    # Filter data for the optimal timeframe
    optimal_start_date = '2012-06-01'
    optimal_end_date = '2014-02-27'
    data_filtered = data[(data['Date'] >= optimal_start_date) & (data['Date'] <= optimal_end_date)]

    # Create new columns for day, month, and year
    data_filtered['Day'] = data_filtered['Date'].dt.day
    data_filtered['Month'] = data_filtered['Date'].dt.month
    data_filtered['Year'] = data_filtered['Date'].dt.year

    # Filter out rows with zero daily consumption
    df_filtered = data_filtered[data_filtered['Daily_Consumption_KWH'] > 0]

    # Group by day, month, and year and calculate the sum of Daily_Consumption_KWH and the house count
    grouped = df_filtered.groupby(['Year', 'Month', 'Day']).agg({
        'Date': 'first',  # Keep the first date in each group
        'CC': 'first',    # assuming taking the first value for these columns is okay
        'RR': 'first',
        'TG': 'first',
        'HU': 'first',
        'SS': 'first',
        'PP': 'first',
        'SD': 'first',
        'QQ': 'first',
        'Daily_Consumption_KWH': 'sum',
        'LCLid': 'nunique',  # count of unique house ids
        'Season': 'first'    # assuming Season is the same across the day
    }).reset_index()

    # Rename columns
    grouped.rename(columns={
        'CC': 'Cloud cover (oktas)',
        'RR': 'Precipitation (0.1 mm)',
        'TG': 'Temperature (0.1°C)',
        'HU': 'Humidity (1%)',
        'SS': 'Sunshine (0.1 hrs)',
        'PP': 'Sea level (0.1 hPa)',
        'SD': 'Snow depth (1 cm)',
        'QQ': 'Radiation (W/m2)',
        'Daily_Consumption_KWH': 'Energy (KWh)',
        'LCLid': 'housecount'
    }, inplace=True)

    # Calculate Energy per household
    grouped['Energy_per_household'] = grouped['Energy (KWh)'] / grouped['housecount']

    # Reorder columns to make 'Date' the first column and 'Energy_per_household' before 'Season'
    grouped = grouped[['Date', 'Year', 'Month', 'Day', 'Cloud cover (oktas)', 'Precipitation (0.1 mm)',
                       'Temperature (0.1°C)', 'Humidity (1%)', 'Sunshine (0.1 hrs)', 'Sea level (0.1 hPa)',
                       'Snow depth (1 cm)', 'Radiation (W/m2)', 'Energy (KWh)', 'Energy_per_household',
                       'housecount', 'Season']]

    return grouped

df = dataframe(merge)
df

df.describe()

df_updated = df.merge(new_df, left_on='Date', right_on='Date', how='left')
df_updated

df_hols = df_updated.merge(hols_df, left_on='Date', right_on='Date', how='left')
df_hols

df_hols['DayOfWeek'] = pd.to_datetime(df_hols['Date']).dt.dayofweek  # monday = 0, sunday = 6
df_hols['Isweekend'] = 0          # Initialize the column with default value of 0
df_hols.loc[df_hols['DayOfWeek'].isin([5, 6]), 'Isweekend'] = 1  # 5 and 6 correspond to Sat and Sun
df_hols

df_hols['holidays'] = np.where(df_hols['holiday'].isna(),0,1)
df_hols

new_df_hols = df_hols.copy()
new_df_hols.loc[new_df_hols['Humidity (1%)'] == -9999, 'Humidity (1%)'] = 87

#Drop the unnecessary columns
cols = ['Year', 'Day', 'holiday', 'DayOfWeek']

# loading new dataframe
refined_df = new_df_hols.drop(columns=cols)
refined_df

#save the combined data frame
refined_df.to_csv('/content/drive/My Drive/refined_df3.csv', index=False)

"""**CREATING COMPOSITE VARIABLES FOR WEATHER NUANCES**"""

# Calculate HDD and CDD
base_temp = 180


refined_df['HDD'] = np.maximum(0, base_temp - refined_df['Temperature (0.1°C)'])
refined_df['CDD'] = np.maximum(0, refined_df['Temperature (0.1°C)'] - base_temp)

# Calculate Thermal Humidity Index
refined_df['THI'] = df['Temperature (0.1°C)'] + (0.55 * ((0.55 * refined_df['Humidity (1%)'] - 10) / 100) * (refined_df['Temperature (0.1°C)'] - 14.5))

# Calculate Wind Chill Index
refined_df['WCI'] = 13.12 + 0.6215 * refined_df['Temperature (0.1°C)'] - 11.37 * refined_df['wspd']**0.16 + 0.3965 * refined_df['Temperature (0.1°C)'] * refined_df['wspd']**0.16

# Calculate Discomfort Index (DI)
refined_df['DI'] = 0.5 * (refined_df['Tmax'] + refined_df['Tmin']) + 0.1 * refined_df['Humidity (1%)'] - 1.7

# Calculate Effective Temperature (ET)
refined_df['ET'] = refined_df['Temperature (0.1°C)'] - ((10 - refined_df['Humidity (1%)']) / 5) * (refined_df['Temperature (0.1°C)'] - 10) - refined_df['wspd'] / 2

refined_df

"""**CREATING LAGGED VARIABLES FOR DAY AHEAD FORECASTS**"""

# List of weather components to lag
lagged_components =  [
    'Cloud cover (oktas)', 'Precipitation (0.1 mm)', 'Temperature (0.1°C)',
    'Humidity (1%)', 'Sunshine (0.1 hrs)', 'Sea level (0.1 hPa)',
    'Snow depth (1 cm)', 'Radiation (W/m2)', 'Energy (KWh)', 'Energy_per_household',
    'Tmin', 'Tmax', 'wdir',	'wspd',	'pres',
    'HDD', 'CDD', 'THI', 'WCI', 'DI', 'ET']

# Create lagged variables
for component in lagged_components:
    refined_df[f'{component}_lag_1'] = refined_df[component].shift(1)

# Drop columns with NaN values
refined_df_lagged = refined_df.dropna()


refined_df_lagged

refined_df_lagged['Humidity (1%)'].describe()

refined_df_lagged.isnull().sum()

refined_df_lagged.columns

#variables
lagged_variables = [
    'Energy (KWh)_lag_1',
    'Energy_per_household_lag_1',
    'Tmin_lag_1',
    'Tmax_lag_1',
    'HDD_lag_1',
    'CDD_lag_1',
    'THI_lag_1',
    'WCI_lag_1',
    'DI_lag_1',
    'ET_lag_1'
]

composite_variables = ['HDD', 'CDD', 'THI', 'WCI', 'DI', 'ET','Isweekend',
                       'Season', 'holidays']

main_variables = [
    'Cloud cover (oktas)', 'Precipitation (0.1 mm)', 'Temperature (0.1°C)',
    'Humidity (1%)', 'Sunshine (0.1 hrs)', 'Sea level (0.1 hPa)',
    'Snow depth (1 cm)', 'Radiation (W/m2)', 'Energy_per_household',
    'Tmin', 'Tmax', 'holidays', 'wdir',	'wspd',	'pres', 'housecount']

def plot_compare(data):
    # Ensure the 'Date' column is datetime type
    #data['Date'] = pd.to_datetime(data['Date'])

    # Set 'Date' as the index for time series plotting
    #data.set_index('Date', inplace=True)

    # List of variables to compare with Energy (KWh)
    main_variables = [
    'Cloud cover (oktas)', 'Precipitation (0.1 mm)', 'Temperature (0.1°C)',
    'Humidity (1%)', 'Sunshine (0.1 hrs)', 'Sea level (0.1 hPa)',
    'Snow depth (1 cm)', 'Radiation (W/m2)', 'Energy_per_household',
    'Tmin', 'Tmax', 'holidays', 'wdir',	'wspd',	'pres', 'housecount']

    # Define number of rows and columns for the grid
    num_vars = len(main_variables)
    cols = 4
    rows = (num_vars + cols - 1) // cols  # Ensure enough rows

    # Create subplots
    plt.style.use('seaborn')
    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(40, 5 * rows), dpi=120, sharex=True)
    axes = axes.flatten()  # Flatten to easily iterate over

    for i, var in enumerate(main_variables):
        ax1 = axes[i]
        ax2 = ax1.twinx()

        # Plot Energy (KWh) on primary y-axis
        ax1.plot(data.index, data['Energy (KWh)'], label='Energy (KWh)', color='blue')
        ax1.set_ylabel('Energy (KWh)', color='blue', fontsize=14, fontweight='bold')
        ax1.tick_params(axis='y', labelcolor='blue', labelsize=10, width=2)

        # Plot the variable on secondary y-axis
        ax2.plot(data.index, data[var], label=var, color='green')
        ax2.set_ylabel(var, color='green', fontsize=14, fontweight='bold')
        ax2.tick_params(axis='y', labelcolor='green', labelsize=10, width=2)

        # Set title
        ax1.set_title(f'Energy vs {var}',fontsize=16, fontweight='bold')

        # Add legend
        lines, labels = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines + lines2, labels + labels2, loc='upper left')

    # Remove any empty subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

  # Rotate x-axis labels for better visibility
    for ax in axes:
        plt.sca(ax)
        plt.xticks(rotation=45)

    plt.xlabel('Date')
    plt.tight_layout()
    plt.show()

plot_compare(refined_df_lagged)

def plot_composite(data):
    # Ensure the 'Date' column is datetime type
    #data['Date'] = pd.to_datetime(data['Date'])

    # Set 'Date' as the index for time series plotting
    #data.set_index('Date', inplace=True)

    # List of variables to compare with Energy (KWh)
    main_variables = composite_variables

    # Define number of rows and columns for the grid
    num_vars = len(main_variables)
    cols = 3
    rows = (num_vars + cols - 1) // cols  # Ensure enough rows

    # Create subplots
    plt.style.use('seaborn')
    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(40, 5 * rows), dpi=120, sharex=True)
    axes = axes.flatten()  # Flatten to easily iterate over

    for i, var in enumerate(main_variables):
        ax1 = axes[i]
        ax2 = ax1.twinx()

        # Plot Energy (KWh) on primary y-axis
        ax1.plot(data.index, data['Energy (KWh)'], label='Energy (KWh)', color='blue')
        ax1.set_ylabel('Energy (KWh)', color='blue', fontsize=14, fontweight='bold')
        ax1.tick_params(axis='y', labelcolor='blue', labelsize=10, width=2)

        # Plot the variable on secondary y-axis
        ax2.plot(data.index, data[var], label=var, color='green')
        ax2.set_ylabel(var, color='green', fontsize=14, fontweight='bold')
        ax2.tick_params(axis='y', labelcolor='green', labelsize=10, width=2)

        # Set title
        ax1.set_title(f'Energy vs {var}',fontsize=16, fontweight='bold')

        # Add legend
        lines, labels = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines + lines2, labels + labels2, loc='upper left')

    # Remove any empty subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

  # Rotate x-axis labels for better visibility
    for ax in axes:
        plt.sca(ax)
        plt.xticks(rotation=45)

    plt.xlabel('Date')
    plt.tight_layout()
    plt.show()

plot_composite(refined_df_lagged)

def plot_lagged(data):
    # Ensure the 'Date' column is datetime type
    #data['Date'] = pd.to_datetime(data['Date'])

    # Set 'Date' as the index for time series plotting
    #data.set_index('Date', inplace=True)

    # List of variables to compare with Energy (KWh)
    main_variables = lagged_variables

    # Define number of rows and columns for the grid
    num_vars = len(main_variables)
    cols = 5
    rows = (num_vars + cols - 1) // cols  # Ensure enough rows

    # Create subplots
    plt.style.use('seaborn')
    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(40, 5 * rows), dpi=120, sharex=True)
    axes = axes.flatten()  # Flatten to easily iterate over

    for i, var in enumerate(main_variables):
        ax1 = axes[i]
        ax2 = ax1.twinx()

        # Plot Energy (KWh) on primary y-axis
        ax1.plot(data.index, data['Energy (KWh)'], label='Energy (KWh)', color='blue')
        ax1.set_ylabel('Energy (KWh)', color='blue', fontsize=14, fontweight='bold')
        ax1.tick_params(axis='y', labelcolor='blue', labelsize=10, width=2)

        # Plot the variable on secondary y-axis
        ax2.plot(data.index, data[var], label=var, color='green')
        ax2.set_ylabel(var, color='green', fontsize=14, fontweight='bold')
        ax2.tick_params(axis='y', labelcolor='green', labelsize=10, width=2)

        # Set title
        ax1.set_title(f'Energy vs {var}',fontsize=16, fontweight='bold')

        # Add legend
        lines, labels = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines + lines2, labels + labels2, loc='upper left')

    # Remove any empty subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

  # Rotate x-axis labels for better visibility
    for ax in axes:
        plt.sca(ax)
        plt.xticks(rotation=45)

    plt.xlabel('Date')
    plt.tight_layout()
    plt.show()

plot_lagged(refined_df_lagged)

daily_features = [
     'Cloud cover (oktas)', 'Precipitation (0.1 mm)',
    'Temperature (0.1°C)', 'Humidity (1%)', 'Sunshine (0.1 hrs)',
    'Sea level (0.1 hPa)', 'Snow depth (1 cm)', 'Radiation (W/m2)',
    'housecount', 'Season', 'wdir', 'wspd', 'pres',
    'Tmin', 'Tmax', 'Isweekend', 'holidays'
]

composite_features = [
    'HDD', 'CDD', 'THI', 'WCI', 'DI', 'ET'
]

lagged_features = [
    'Cloud cover (oktas)_lag_1', 'Precipitation (0.1 mm)_lag_1',
    'Temperature (0.1°C)_lag_1', 'Humidity (1%)_lag_1',
    'Sunshine (0.1 hrs)_lag_1', 'Sea level (0.1 hPa)_lag_1',
    'Snow depth (1 cm)_lag_1', 'Radiation (W/m2)_lag_1',
    'Energy (KWh)_lag_1', 'Energy_per_household_lag_1',
    'Tmin_lag_1', 'Tmax_lag_1', 'wdir_lag_1', 'wspd_lag_1',
    'pres_lag_1', 'HDD_lag_1', 'CDD_lag_1', 'THI_lag_1',
    'WCI_lag_1', 'DI_lag_1', 'ET_lag_1'
]

output_features = ['Energy (KWh)', 'Energy_per_household']

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

def plot_output_feature_correlation(data, output_features, feature_list, title):
    # Encode 'Season' column if present
    if 'Season' in feature_list:
        encoder = LabelEncoder()
        data['Season_encoded'] = encoder.fit_transform(data['Season'])
        feature_list.remove('Season')
        feature_list.append('Season_encoded')

    # Select only the features and output features for correlation
    selected_features = feature_list + output_features
    corr = data[selected_features].corr()

    # Create a heatmap
    plt.figure(figsize=(60, 30), dpi=120)
    ax = sns.heatmap(corr, annot=True, cmap='coolwarm', center=0,
                     annot_kws={'fontsize': 20, 'fontweight': 'bold'})

    # Set title
    plt.title('Correlation Matrix', fontsize=30, fontweight='bold')

    # Make axis labels bold
    plt.setp(ax.get_xticklabels(), fontsize=20, fontweight='bold')
    plt.setp(ax.get_yticklabels(), fontsize=20, fontweight='bold')

    # Make tick labels bold
    ax.tick_params(axis='both', which='major', labelsize=20, width=2)

    # Rotate x-axis labels
    plt.xticks(rotation=90)  # Adjust the rotation angle as needed

    # Show plot
    plt.show()

# dailyfeatures
plot_output_feature_correlation(refined_df_lagged, output_features, daily_features, "Correlation with Daily Features")

plot_output_feature_correlation(refined_df_lagged, output_features, composite_features, "Correlation with Composite Features")

plot_output_feature_correlation(refined_df_lagged, output_features, lagged_features, "Correlation with Lagged Features")

"""**MODEL TESTING**
Using regression just for testing purposes
"""

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Encode categorical variable 'Season'
label_encoder = LabelEncoder()
refined_df_lagged['Season'] = label_encoder.fit_transform(refined_df_lagged['Season'])

# Define features and target
features = daily_features

target = 'Energy (KWh)'

# Split data into training and testing sets
X = refined_df_lagged[features]
y = refined_df_lagged[target]


# Convert all columns to numeric (ignore errors for non-numeric columns)
X = X.apply(pd.to_numeric, errors='coerce')
X = X.dropna()  # Drop columns with NaN after conversion

# Check and ensure y is numeric
y = pd.to_numeric(y, errors='coerce').dropna()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Hyperparameter tuning with Grid Search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best model parameters
best_params = grid_search.best_params_
print(f'Best Model Parameters: {best_params}')

# Best model
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

# Evaluate the best model
mae_best = mean_absolute_error(y_test, y_pred_best)
mse_best = mean_squared_error(y_test, y_pred_best)
rmse_best = mean_squared_error(y_test, y_pred_best, squared=False)
r2_best = r2_score(y_test, y_pred_best)
mape_best = (abs(y_test - y_pred_best) / y_test).mean() * 100

print(f'Best Model Mean Absolute Error: {mae_best}')
print(f'Best Model Mean Squared Error: {mse_best}')
print(f'Best Model R-squared: {r2_best}')
print(f'Best Model Mean Absolute Percentage Error: {mape_best}%')
print(f'Best Model Root Mean Squared Error: {rmse_best}')

# Cross-validation to get a robust estimate of model performance
cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2')
print(f'Cross-validated R-squared: {cv_scores.mean()}')

# Plotting predicted vs actual values
plt.figure(figsize=(10, 6))

# Scatter plot
plt.scatter(y_test, y_pred_best, color='blue', alpha=0.5, label='Predicted vs Actual')

# Line plot for perfect predictions
plt.plot(y_test, y_test, color='red', label='Perfect Prediction')

plt.title('Predicted vs Actual')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.legend()

plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import Lasso
from sklearn.feature_selection import RFE
from sklearn.base import BaseEstimator, RegressorMixin
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Feature engineering: lag features, rolling statistics, interaction features
df['Temp_Lag1'] = df['Temperature (0.1°C)'].shift(1)
df['Temp_RollingMean7'] = df['Temperature (0.1°C)'].rolling(window=7).mean()
df['Temp_Humidity_Interaction'] = df['Temperature (0.1°C)'] * df['Humidity (1%)']
df = df.dropna()

# Normalize features and target separately
feature_scaler = MinMaxScaler()
refined_df_lagged[features] = feature_scaler.fit_transform(refined_df_lagged[features].apply(pd.to_numeric, errors='coerce').dropna())
target_scaler = MinMaxScaler()
refined_df_lagged[target] = target_scaler.fit_transform(refined_df_lagged[[target]])

# Create sequences for time series
def create_sequences(data, features, target, sequence_length):
    X = []
    y = []
    for i in range(len(data) - sequence_length):
        X.append(data[features].iloc[i:i+sequence_length].values)
        y.append(data[target].iloc[i+sequence_length])
    return np.array(X), np.array(y)

sequence_length = 300
X, y = create_sequences(refined_df_lagged, features, target, sequence_length)

# Split data into training and testing sets
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Define the GRU model
def create_model(units=20, dropout_rate=0.2, learning_rate=0.001):
    model = Sequential()
    model.add(GRU(units=units, return_sequences=True, input_shape=(sequence_length, len(features)), kernel_regularizer='l2'))
    model.add(GRU(units=units, kernel_regularizer='l2'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')
    return model

# Custom Keras wrapper
class KerasRegressor(BaseEstimator, RegressorMixin):
    def __init__(self, units=50, dropout_rate=0.2, learning_rate=0.001, epochs=20, batch_size=32):
        self.units = units
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.batch_size = batch_size
        self.model_ = None

    def fit(self, X, y):
        self.model_ = create_model(self.units, self.dropout_rate, self.learning_rate)
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        self.history_ = self.model_.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0, validation_split=0.2, callbacks=[early_stopping])
        return self

    def predict(self, X):
        return self.model_.predict(X)

    def score(self, X, y):
        y_pred = self.predict(X)
        return -mean_squared_error(y, y_pred)

# Define the grid of hyperparameters to search
param_grid = {
    'units': [25, 50, 100, 150, 250],
    'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5],
    'learning_rate': [0.01, 0.001, 0.0001, 0.00001],
    'batch_size': [16, 32, 64]
}

# Perform grid search with TimeSeriesSplit
grid = GridSearchCV(estimator=KerasRegressor(epochs=20), param_grid=param_grid, cv=TimeSeriesSplit(n_splits=3), n_jobs=-1)
grid_result = grid.fit(X_train, y_train)

# Print the best parameters
print(f"Best Parameters: {grid_result.best_params_}")

# Best model evaluation
best_model = grid_result.best_estimator_

# Plot training & validation loss values
plt.figure(figsize=(14, 5))
plt.plot(best_model.history_.history['loss'], label='Training Loss')
plt.plot(best_model.history_.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

y_pred = best_model.predict(X_test)

# Rescale the predictions and true values back to the original scale
y_test_rescaled = target_scaler.inverse_transform(y_test.reshape(-1, 1))
y_pred_rescaled = target_scaler.inverse_transform(y_pred)

# Evaluate the model
mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)
mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_rescaled, y_pred_rescaled)

print(f'Best Model Mean Absolute Error: {mae}')
print(f'Best Model Mean Squared Error: {mse}')
print(f'Best Model Root Mean Squared Error (RMSE): {rmse}')
print(f'Best Model R-squared: {r2}')

!pip install shap

print(f'Min Energy: {df["Energy (KWh)"].min()}')
print(f'Max Energy: {df["Energy (KWh)"].max()}')
print(f'Mean Energy: {df["Energy (KWh)"].mean()}')
